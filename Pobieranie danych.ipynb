{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f22fdb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from afinn import Afinn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import dateutil.parser as parser\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1714f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maksymalna ilosc tweetow\n",
    "maxTweets = 1000000000000000000\n",
    "# slowo kluczowe\n",
    "\n",
    "keyword = '(COVID OR covid OR Covid OR covid19 OR COVID19 OR COVID-19 OR Covid-19 OR covid-19 OR Corona VÃ­rus OR corona virus OR SARS-CoV-2 OR coronavirus)'\n",
    "#tworzenie pliku, w ktorym beda zapisane tweety\n",
    "csvFile = open('tweetCovid_2020.csv', 'a', newline='', encoding='utf8')\n",
    "#edytor csv\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['Datetime', 'TweetID', 'Text', 'UserID', 'likeCount', 'lang', 'hastags', 'coordinates', 'place'])\n",
    "\n",
    "# zapisanie tweetow do csv\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' lang:en since:2019-12-15 until:2021-01-01 -filter:links -filter:replies').get_items()):\n",
    "        if i > maxTweets :\n",
    "            break  \n",
    "        csvWriter.writerow([tweet.date, tweet.id, tweet.content, tweet.user.id, tweet.likeCount, tweet.lang, tweet.hashtags, tweet.coordinates, tweet.place])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8dbeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tworzenie pliku, w ktorym beda zapisane tweety\n",
    "csvFile = open('tweetCovid_2021.csv', 'a', newline='', encoding='utf8')\n",
    "#edytor csv\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['Datetime', 'TweetID', 'Text', 'UserID', 'likeCount', 'lang', 'hastags', 'coordinates', 'place'])\n",
    "\n",
    "# zapisanie tweetow do csv\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' lang:en since:2021-01-01 until:2022-04-01 -filter:links -filter:replies').get_items()):\n",
    "        if i > maxTweets :\n",
    "            break  \n",
    "        csvWriter.writerow([tweet.date, tweet.id, tweet.content, tweet.user.id, tweet.likeCount, tweet.lang, tweet.hashtags, tweet.coordinates, tweet.place])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1e122b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: DtypeWarning: Columns (1,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "tweet2020 = pd.read_csv('tweetCovid_2020.csv')\n",
    "tweet2021 = pd.read_csv('tweetCovid_2021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44f8d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2020 [['date', 'time']] = tweet2020 ['Datetime'].str.split(\" \", expand=True, n=-1)\n",
    "tweet2021 [['date', 'time']] = tweet2021 ['Datetime'].str.split(\" \", expand=True, n=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "946d1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zmiana formatu czasu\n",
    "tweet2020['time'] = pd.to_datetime(tweet2020['time']).dt.strftime(\"%H:%M\")\n",
    "tweet2021['time'] = pd.to_datetime(tweet2021['time']).dt.strftime(\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c43e1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usuniecie kolumny Datetime\n",
    "tweet2020.drop('Datetime', axis=1, inplace=True)\n",
    "tweet2021.drop('Datetime', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b6fbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapis do pliku csv\n",
    "tweet2020.to_csv('tweet2020.csv', sep=',', encoding='utf-8', index=False)\n",
    "tweet2021.to_csv('tweet2021.csv', sep=',', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
